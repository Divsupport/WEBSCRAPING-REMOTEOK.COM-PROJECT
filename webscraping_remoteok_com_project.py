# -*- coding: utf-8 -*-
"""WEBSCRAPING REMOTEOK.COM PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wR7cmpTZ-DEcdYSHQJqrP0sB89RW0Iu9

# WEB SCRAPING REMOTEOK.COM
"""

# This is a Real job listing site: https://remoteok.com to scrape remote job postings in tech.
# Project Goal is to Extract job titles, companies, and job tags from the RemoteOK site and save them in a CSV.

import requests                                        # The requests will help me fetch the html site
from bs4 import BeautifulSoup                    # The BeautifulSoup will help me navigate the html
import pandas as pd                        # the pandas will help me to handle and save it in a tabular data

# step 1: is to source the URL

url = 'https://remoteok.com/remote-dev-jobs'

# step 2: is to set custom headers — specifically a User-Agent string — to make my script mimic a real browser like Google Chrome.

headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/123.0.0.0 Safari/537.36'
    )
}

# step 3: is to Fetch the webpage by sending a request to the website and storing the raw HTML content in html.

response = requests.get(url, headers=headers)
html = response.text

# Step 4: The 'soup' object lets me search and extract HTML contents.

soup = BeautifulSoup(html, 'html.parser')

# step 5: I want to Find all job class using Find_all

jobs = soup.find_all('tr', class_='job')


print(soup.prettify())

# Step 6: Extract details from each job
job_data = []  # I created an empty list to store each job's data

for job in jobs:
    try:
        # I extracted the job title from the <h2> tag with itemprop='title'
        title = job.find('h2', itemprop='title').text.strip()

        # I extracted the company name from the <h3> tag with itemprop='name'
        company = job.find('h3', itemprop='name').text.strip()

        # I used Find_all to look for all the tags listed under the job (like Python, React, etc.)
        tags = job.find_all('div', class_='tag')

        # I extracted only the visible text from each HTML tag (like "Python", "React") and storing all those texts in a list.
        tag_list = [tag.text.strip() for tag in tags]

        # Append all extracted details as a dictionary to the job_data list
        job_data.append({
            'Title': title,
            'Company': company,
            'Tags': ', '.join(tag_list)  # It will Join the tags into a comma-separated string
        })
    except:
        # Skip this job listing if any information is missing or an error occurs
        continue

# It will each job's information nicely formatted
for job in job_data:
    print(f"Title: {job['Title']}")     # Print job title
    print(f"Company: {job['Company']}") # Print company name
    print(f"Tags: {job['Tags']}")       # Print the associated tags
    print('-' * 40)                     # Print a separator line

# Step 7: I will Convert this into DataFrame and save as a CSV File
df = pd.DataFrame(job_data)
df.to_csv('remote_jobs.csv', index=False)

print("✅ Data saved to remote_jobs.csv")

from google.colab import drive
drive.mount('/content/drive')





























































